{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from typing import List, Optional\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import wandb\n",
    "import logging\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets.my_dataset import MyDataset\n",
    "from utils.misc import import_str\n",
    "from models.trainers import BasicTrainer\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c717672bf7f74b44",
   "metadata": {},
   "source": [
    "data_root = \"/mnt/e/Output/background/149\"\n",
    "\n",
    "cfg = OmegaConf.load(os.path.join(data_root, \"config.yaml\"))\n",
    "%cd /home/a/drivestudio\n",
    "cfg.data.data_root = \"data/waymo/processed/training\"\n",
    "\n",
    "dataset = MyDataset(cfg.data)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading SMPL: 100%|██████████| 199/199 [00:00<00:00, 696.20it/s] \n",
      "Loading lidar:   0%|          | 0/199 [00:00<?, ?it/s]/home/a/drivestudio/datasets/waymo/waymo_sourceloader.py:411: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  lidar_origins = torch.from_numpy(lidar_info[:, :3]).float()  # 提取原点\n",
      "Loading lidar: 100%|██████████| 199/199 [00:17<00:00, 11.11it/s]\n",
      "Projecting lidar pts on images for camera front_camera: 100%|██████████| 199/199 [00:05<00:00, 34.76it/s]\n",
      "Projecting lidar pts on images for camera front_left_camera: 100%|██████████| 199/199 [00:05<00:00, 35.99it/s]\n",
      "Projecting lidar pts on images for camera front_right_camera: 100%|██████████| 199/199 [00:05<00:00, 36.89it/s]\n",
      "Projecting lidar pts on images for camera left_camera: 100%|██████████| 199/199 [00:05<00:00, 34.96it/s]\n",
      "Projecting lidar pts on images for camera right_camera: 100%|██████████| 199/199 [00:07<00:00, 27.68it/s]\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "d8c90179b4b29f22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:27:01.853194Z",
     "start_time": "2024-12-22T09:26:58.000397Z"
    }
   },
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "trainer = import_str(cfg.trainer.type)(\n",
    "    **cfg.trainer,\n",
    "    num_timesteps=dataset.num_img_timesteps,\n",
    "    model_config=cfg.model,\n",
    "    num_train_images=len(dataset.train_image_set),\n",
    "    num_full_images=len(dataset.full_image_set),\n",
    "    test_set_indices=dataset.test_timesteps,\n",
    "    scene_aabb=dataset.get_aabb().reshape(2, 3),\n",
    "    device=device\n",
    "\n",
    ")\n",
    "# ckpt_path = os.path.join(data_root, \"checkpoint_final.pth\")\n",
    "#\n",
    "# trainer.resume_from_checkpoint(\n",
    "#     ckpt_path=ckpt_path,\n",
    "#     load_only_model=True\n",
    "# )"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/a/anaconda3/envs/drivestudio/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/a/anaconda3/envs/drivestudio/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "82d2759cc7170d8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:27:02.141572Z",
     "start_time": "2024-12-22T09:27:02.104297Z"
    }
   },
   "source": [
    "import pyiqa\n",
    "import random\n",
    "\n",
    "device = trainer.device\n",
    "render_dir = os.path.join(cfg.log_dir, f\"render\")\n",
    "pred_dir = os.path.join(cfg.log_dir, f\"pred\")\n",
    "\n",
    "os.makedirs(render_dir, exist_ok=True)\n",
    "for file in os.listdir(render_dir):\n",
    "    if file.endswith(\".png\"):\n",
    "        os.remove(os.path.join(render_dir, file))\n",
    "#\n",
    "# os.makedirs(pred_dir, exist_ok=True)\n",
    "# for file in os.listdir(pred_dir):\n",
    "#     if file.endswith(\".png\"):\n",
    "#         os.remove(os.path.join(pred_dir, file))\n",
    "\n",
    "# iqa_metric = pyiqa.create_metric('brisque', device=device)"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "564ba730eb58c829",
   "metadata": {},
   "source": [
    "image_info_list, cam_info_list = [], []\n",
    "cam2worlds = torch.load(\"notebook/data/cam2worlds.pth\")\n",
    "intrinsics = torch.load(\"notebook/data/intrinsics.pth\")\n",
    "width, height = 960, 640\n",
    "from utils.visualization import to8b\n",
    "import cv2\n",
    "import random\n",
    "\n",
    "for idx in range(len(cam2worlds)):\n",
    "    c2w = cam2worlds[idx]\n",
    "    intrinsic = intrinsics[idx]\n",
    "\n",
    "\n",
    "    cam_info = {\n",
    "        \"camera_to_world\": c2w.to(device),\n",
    "        \"intrinsics\": intrinsic.to(device),\n",
    "        \"height\": torch.tensor(height, dtype=torch.long, device=device),\n",
    "        \"width\": torch.tensor(width, dtype=torch.long, device=device),\n",
    "    }\n",
    "\n",
    "    x, y = torch.meshgrid(\n",
    "        torch.arange(width),\n",
    "        torch.arange(height),\n",
    "        indexing=\"xy\",\n",
    "    )\n",
    "    x, y = x.flatten(), y.flatten()\n",
    "    x, y = x.to(device), y.to(device)\n",
    "\n",
    "    pixel_coords = (\n",
    "        torch.stack([y / height, x / width], dim=-1)\n",
    "        .float()\n",
    "        .reshape(height, width, 2)\n",
    "    )\n",
    "    from datasets.base.pixel_source import get_rays\n",
    "\n",
    "    intrinsic = intrinsic * dataset.pixel_source.downscale_factor\n",
    "    intrinsic[2, 2] = 1.0\n",
    "    intrinsic = intrinsic.to(device)\n",
    "    c2w = c2w.to(device)\n",
    "    origins, viewdirs, direction_norm = get_rays(x, y, c2w, intrinsic)\n",
    "\n",
    "    viewdirs = viewdirs.reshape(height, width, 3)\n",
    "\n",
    "    image_id = torch.full(\n",
    "        (height, width),\n",
    "        0,\n",
    "        dtype=torch.long,\n",
    "    )\n",
    "\n",
    "    time_step = dataset.pixel_source.normalized_time.shape\n",
    "    t = random.randint(0, time_step[0] - 1)\n",
    "\n",
    "    normalized_time = torch.full(\n",
    "        (height, width),\n",
    "        dataset.pixel_source.normalized_time[t],\n",
    "        dtype=torch.float32,\n",
    "    )\n",
    "\n",
    "    image_info = {\n",
    "        \"origins\": origins.to(device),\n",
    "        \"direction_norm\": direction_norm.to(device),\n",
    "        \"viewdirs\": viewdirs.to(device),\n",
    "        \"img_idx\": image_id.to(device),\n",
    "        \"pixel_coords\": pixel_coords.to(device),\n",
    "        \"normed_time\": normalized_time.to(device),\n",
    "    }\n",
    "    # train_step_camera_downscale = trainer._get_downscale_factor()\n",
    "    # image_info, cam_info = dataset.train_image_set[idx]\n",
    "    # for k, v in image_info.items():\n",
    "    #     if isinstance(v, torch.Tensor):\n",
    "    #         image_info[k] = v.cuda(non_blocking=True)\n",
    "    # for k, v in cam_info.items():\n",
    "    #     if isinstance(v, torch.Tensor):\n",
    "    #         cam_info[k] = v.cuda(non_blocking=True)\n",
    "\n",
    "    output = trainer(image_info, cam_info, False)\n",
    "\n",
    "    sky_mask = output[\"opacity\"].cpu().detach()\n",
    "    sky_mask = sky_mask.reshape(height, width)\n",
    "    sky_mask = (sky_mask > 0.5).float()\n",
    "\n",
    "    image_info[\"sky_masks\"] = sky_mask.to(device)\n",
    "\n",
    "    img = to8b(output[\"rgb\"])\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # iqa_img = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).to(device)\n",
    "\n",
    "    # score = iqa_metric(iqa_img)\n",
    "    # if score > 60:\n",
    "    #     continue\n",
    "\n",
    "    save_path = os.path.join(render_dir, f\"{idx:03d}.png\")\n",
    "    cv2.imwrite(save_path, img)\n",
    "\n",
    "    image_info_list.append(image_info)\n",
    "    cam_info_list.append(cam_info)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "88effc3cc46d105b",
   "metadata": {},
   "source": [
    "for idx in range(len(cam2worlds)):\n",
    "    render_img_path = os.path.join(render_dir, f\"{idx:03d}.png\")\n",
    "    pred_img_path = os.path.join(pred_dir, f\"{idx:03d}.png\")\n",
    "\n",
    "    if not (os.path.exists(render_img_path) and os.path.exists(pred_img_path)):\n",
    "        print(pred_img_path)\n",
    "        continue\n",
    "\n",
    "    pred_img = cv2.imread(pred_img_path)\n",
    "    pred_img = cv2.cvtColor(pred_img, cv2.COLOR_RGB2BGR)\n",
    "    pred_img = torch.from_numpy(pred_img).float() / 255.0\n",
    "\n",
    "    image_info_list[idx][\"pixels\"] = pred_img.to(device)\n",
    "\n",
    "to_delete = [idx for idx, image_info in enumerate(image_info_list)  if \"pixels\" not in image_info]\n",
    "for idx in reversed(to_delete):\n",
    "    del image_info_list[idx]\n",
    "    del cam_info_list[idx]\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "dataset.load_fake_gt(image_info_list, cam_info_list, True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# torch.save(image_info_list, \"notebook/data/image_info_list.pth\")\n",
    "# torch.save(cam_info_list, \"notebook/data/cam_info_list.pth\")\n",
    "image_info_list = torch.load(\"notebook/data/image_info_list.pth\")\n",
    "cam_info_list = torch.load(\"notebook/data/cam_info_list.pth\")\n",
    "dataset.load_fake_gt(image_info_list, cam_info_list, True)\n"
   ],
   "id": "2609918a24df7cb6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-22T09:27:47.634080Z",
     "start_time": "2024-12-22T09:27:42.127631Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trainer.init_gaussians_from_dataset(dataset=dataset)\n",
    "trainer.initialize_optimizer()\n",
    "trainer.init_viewer(8080)"
   ],
   "id": "138016f2f9fe9bb7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "╭─────────────── \u001B[1mviser\u001B[0m ───────────────╮\n",
       "│             ╷                       │\n",
       "│   HTTP      │ http://0.0.0.0:8080   │\n",
       "│   Websocket │ ws://0.0.0.0:8080     │\n",
       "│             ╵                       │\n",
       "╰─────────────────────────────────────╯\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">╭─────────────── <span style=\"font-weight: bold\">viser</span> ───────────────╮\n",
       "│             ╷                       │\n",
       "│   HTTP      │ http://0.0.0.0:8080   │\n",
       "│   Websocket │ ws://0.0.0.0:8080     │\n",
       "│             ╵                       │\n",
       "╰─────────────────────────────────────╯\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "c9cc5b994cc66d1d",
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-12-22T09:32:13.220901800Z",
     "start_time": "2024-12-22T09:27:49.585036Z"
    }
   },
   "source": [
    "\n",
    "for step in range(0, 10000):\n",
    "    #----------------------------------------------------------------------------\n",
    "    #----------------------------  training step  -------------------------------\n",
    "    if step % 100 == 0:\n",
    "        print(step)\n",
    "    # prepare for training\n",
    "    trainer.set_train()\n",
    "    trainer.preprocess_per_train_step(step=step)\n",
    "\n",
    "    trainer.optimizer_zero_grad() # zero grad\n",
    "    # get data\n",
    "    use_fake_gt = random.random() < 0.\n",
    "    if use_fake_gt and step > 500:\n",
    "        image_infos, cam_infos = dataset.fake_gt_next()\n",
    "    else:\n",
    "        train_step_camera_downscale = trainer._get_downscale_factor()\n",
    "        image_infos, cam_infos = dataset.train_image_set.next(train_step_camera_downscale)\n",
    "    for k, v in image_infos.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            image_infos[k] = v.cuda(non_blocking=True)\n",
    "    for k, v in cam_infos.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            cam_infos[k] = v.cuda(non_blocking=True)\n",
    "\n",
    "    # forward & backward\n",
    "    outputs = trainer(image_infos, cam_infos, ~ use_fake_gt)\n",
    "    trainer.update_visibility_filter()\n",
    "    loss_dict = trainer.compute_losses(\n",
    "        outputs=outputs,\n",
    "        image_infos=image_infos,\n",
    "        cam_infos=cam_infos,\n",
    "    )\n",
    "    # check nan or inf\n",
    "    for k, v in loss_dict.items():\n",
    "        if torch.isnan(v).any():\n",
    "            raise ValueError(f\"NaN detected in loss {k} at step {step}\")\n",
    "        if torch.isinf(v).any():\n",
    "            raise ValueError(f\"Inf detected in loss {k} at step {step}\")\n",
    "    trainer.backward(loss_dict)\n",
    "\n",
    "    # after training step\n",
    "    trainer.postprocess_per_train_step(step=step)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "Class Background# current points: 1026728 @ step 600\n",
      "Class Background# left points: 1026728\n",
      "700\n",
      "Class Background# current points: 1026728 @ step 700\n",
      "Class Background# left points: 1026728\n",
      "800\n",
      "Class Background# current points: 1026728 @ step 800\n",
      "Class Background# left points: 1026728\n",
      "900\n",
      "Class Background# current points: 1026728 @ step 900\n",
      "Class Background# left points: 1026728\n",
      "1000\n",
      "Class Background# current points: 1026728 @ step 1000\n",
      "    Split: 13365\n",
      "      Dup: 35137\n",
      "     Cull: 21842\n",
      "Class Background# left points: 1066753\n",
      "1100\n",
      "Class Background# current points: 1066753 @ step 1100\n",
      "    Split: 10797\n",
      "      Dup: 25360\n",
      "     Cull: 5749\n",
      "Class Background# left points: 1107958\n",
      "1200\n",
      "Class Background# current points: 1107958 @ step 1200\n",
      "    Split: 14212\n",
      "      Dup: 35808\n",
      "     Cull: 5073\n",
      "Class Background# left points: 1167117\n",
      "1300\n",
      "Class Background# current points: 1167117 @ step 1300\n",
      "    Split: 12508\n",
      "      Dup: 32871\n",
      "     Cull: 3680\n",
      "Class Background# left points: 1221324\n",
      "1400\n",
      "Class Background# current points: 1221324 @ step 1400\n",
      "    Split: 13178\n",
      "      Dup: 36576\n",
      "     Cull: 4265\n",
      "Class Background# left points: 1279991\n",
      "1500\n",
      "Class Background# current points: 1279991 @ step 1500\n",
      "    Split: 12154\n",
      "      Dup: 33572\n",
      "     Cull: 4914\n",
      "Class Background# left points: 1332957\n",
      "1600\n",
      "Class Background# current points: 1332957 @ step 1600\n",
      "    Split: 12552\n",
      "      Dup: 34752\n",
      "     Cull: 5187\n",
      "Class Background# left points: 1387626\n",
      "1700\n",
      "Class Background# current points: 1387626 @ step 1700\n",
      "    Split: 12914\n",
      "      Dup: 33878\n",
      "     Cull: 4344\n",
      "Class Background# left points: 1442988\n",
      "1800\n",
      "Class Background# current points: 1442988 @ step 1800\n",
      "    Split: 12319\n",
      "      Dup: 31591\n",
      "     Cull: 3531\n",
      "Class Background# left points: 1495686\n",
      "1900\n",
      "Class Background# current points: 1495686 @ step 1900\n",
      "    Split: 12326\n",
      "      Dup: 31120\n",
      "     Cull: 3749\n",
      "Class Background# left points: 1547709\n",
      "2000\n",
      "Class Background# current points: 1547709 @ step 2000\n",
      "    Split: 11381\n",
      "      Dup: 25062\n",
      "     Cull: 4692\n",
      "Class Background# left points: 1590841\n",
      "2100\n",
      "Class Background# current points: 1590841 @ step 2100\n",
      "    Split: 12204\n",
      "      Dup: 29249\n",
      "     Cull: 4238\n",
      "Class Background# left points: 1640260\n",
      "2200\n",
      "Class Background# current points: 1640260 @ step 2200\n",
      "    Split: 11696\n",
      "      Dup: 23456\n",
      "     Cull: 3353\n",
      "Class Background# left points: 1683755\n",
      "2300\n",
      "Class Background# current points: 1683755 @ step 2300\n",
      "    Split: 12379\n",
      "      Dup: 26725\n",
      "     Cull: 3232\n",
      "Class Background# left points: 1732006\n",
      "2400\n",
      "Class Background# current points: 1732006 @ step 2400\n",
      "    Split: 11162\n",
      "      Dup: 23469\n",
      "     Cull: 2770\n",
      "Class Background# left points: 1775029\n",
      "2500\n",
      "Class Background# current points: 1775029 @ step 2500\n",
      "    Split: 10969\n",
      "      Dup: 23424\n",
      "     Cull: 2814\n",
      "Class Background# left points: 1817577\n",
      "2600\n",
      "Class Background# current points: 1817577 @ step 2600\n",
      "    Split: 9109\n",
      "      Dup: 17461\n",
      "     Cull: 2731\n",
      "Class Background# left points: 1850525\n",
      "2700\n",
      "Class Background# current points: 1850525 @ step 2700\n",
      "    Split: 8210\n",
      "      Dup: 14634\n",
      "     Cull: 2543\n",
      "Class Background# left points: 1879036\n",
      "2800\n",
      "Class Background# current points: 1879036 @ step 2800\n",
      "    Split: 9164\n",
      "      Dup: 18768\n",
      "     Cull: 2453\n",
      "Class Background# left points: 1913679\n",
      "2900\n",
      "Class Background# current points: 1913679 @ step 2900\n",
      "    Split: 9319\n",
      "      Dup: 17667\n",
      "     Cull: 2343\n",
      "Class Background# left points: 1947641\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "26f56c4a53d0a32f",
   "metadata": {},
   "source": [
    "trainer.save_checkpoint(\n",
    "    log_dir=cfg.log_dir,\n",
    "    save_only_model=True,\n",
    "    is_final=False,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b65798ca49ef44ee",
   "metadata": {},
   "source": [
    "dataset.pixel_source.camera_data"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f5b82d4b6c122d15",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
